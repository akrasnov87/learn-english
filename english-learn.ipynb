{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fd6e2e4",
   "metadata": {},
   "source": [
    "Просмотр фильмов на оригинальном языке - это популярный и действенный метод прокачаться при изучении иностранных языков. Важно выбрать фильм, который подходит студенту по уровню сложности, т.е. студент понимал 50-70 % диалогов. Чтобы выполнить это условие, преподаватель должен посмотреть фильм и решить, какому уровню он соответствует. Однако это требует больших временных затрат.\n",
    "\n",
    "Требуется разработать ML решение для автоматического определения уровня сложности англоязычных фильмов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f621db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas pysrt nltk pyprind joblib\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956379d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import pysrt\n",
    "import re\n",
    "from time import time\n",
    "from joblib import dump\n",
    "import pyprind\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils.extmath import density\n",
    "\n",
    "# обучим модели\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d724122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# пусть к файлам, где хранятся субтитры\n",
    "PATH='learning/subtitles/*/*.srt'\n",
    "\n",
    "# путь к дополнительным данным для фильмов\n",
    "MOVIES_LABEL_PATH='learning/data/movies_labels.xlsx'\n",
    "\n",
    "# путь к файлу со словами\n",
    "WORDS_PATH='learning/data/words.xlsx'\n",
    "\n",
    "# каталог для сохранение результата\n",
    "OUTPUT_PATH='learning/models/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95bdcb7d",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c873875",
   "metadata": {},
   "source": [
    "Считываем субтитры, которые хранятся в каталоге <b>subtitles</b>, если требуется добавить новые данные, то можно создать дополнительный каталог, куда можно поместить дополнительные файлы.\n",
    "\n",
    "<b>Примечание</b>: нужно также дополнительно в файле <b>movies_labels.xlsx</b> указать уровень знания английского языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# считываем файлы субтитров (файлы были предварительно обработаны: корректность имени)\n",
    "files = []\n",
    "movies = []\n",
    "\n",
    "for file in glob.glob(PATH):\n",
    "    full_path = file.replace('\\\\', '/')\n",
    "    movies.append(os.path.basename(full_path).replace('.srt', ''))\n",
    "    files.append([full_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаём DataFrame\n",
    "df_movies = pd.DataFrame(files, columns=['Path'], index=movies)    \n",
    "print(f'Было найдено {df_movies.shape[0]} фильмов')\n",
    "\n",
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f84fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заранее был обработан файл movies_labels.xlsx, где были удалены дубликаты \n",
    "df_movies_labels = pd.read_excel(MOVIES_LABEL_PATH, index_col=1)\n",
    "df_movies_labels.drop(['id'], axis=1, inplace=True)\n",
    "print(f'Было найдено {df_movies_labels.shape[0]} фильмов')\n",
    "\n",
    "df_movies_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090bc303",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = df_movies.merge(df_movies_labels, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# были найдены фильмы, по которым неизвестна категория\n",
    "df_movies = df_movies[~df_movies['Max_Level'].isna()]\n",
    "\n",
    "print(f'Итоговое количество фильмов для обработки {df_movies.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ea2494",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79682922",
   "metadata": {},
   "source": [
    "<b>Примечание</b>: у некоторых фильмах было указано несколько категорий, в этом случаи я брать максимальное значение (см. колонку Max_Level)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4103a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# используем вспомогательные методы\n",
    "\n",
    "HTML = r'<.*?>' # html тэги меняем на пробел\n",
    "TAG = r'{.*?}' # тэги меняем на пробел\n",
    "COMMENTS = r'[\\(\\[][A-Za-z ]+[\\)\\]]' # комменты в скобках меняем на пробел\n",
    "UPPER = r'[[A-Za-z ]+[\\:\\]]' # указания на того кто говорит (BOBBY:)\n",
    "LETTERS = r'[^a-zA-Z\\'.,!? ]' # все что не буквы меняем на пробел \n",
    "SPACES = r'([ ])\\1+' # повторяющиеся пробелы меняем на один пробел\n",
    "DOTS = r'[\\.]+' # многоточие меняем на точку\n",
    "SYMB = r\"[^\\w\\d'\\s]\" # знаки препинания кроме апострофа\n",
    "\n",
    "def clean_subs(subs):\n",
    "    \"\"\"\n",
    "    Очистка субтитров\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    subs: SubRipFile - объект с информацией о субтитрах\n",
    "\n",
    "    Результат:\n",
    "    ----------\n",
    "    Отформатированная строка\n",
    "    \"\"\"\n",
    "    subs = subs[1:] # удаляем первый рекламный субтитр\n",
    "    txt = re.sub(HTML, ' ', subs.text) # html тэги меняем на пробел\n",
    "    txt = re.sub(COMMENTS, ' ', txt) # комменты в скобках меняем на пробел\n",
    "    txt = re.sub(UPPER, ' ', txt) # указания на того кто говорит (BOBBY:)\n",
    "    txt = re.sub(LETTERS, ' ', txt) # все что не буквы меняем на пробел\n",
    "    txt = re.sub(DOTS, r'.', txt) # многоточие меняем на точку\n",
    "    txt = re.sub(SPACES, r'\\1', txt) # повторяющиеся пробелы меняем на один пробел\n",
    "    txt = re.sub(SYMB, '', txt) # знаки препинания кроме апострофа на пустую строку\n",
    "    txt = re.sub('www', '', txt) # кое-где остаётся www, то же меняем на пустую строку\n",
    "    txt = txt.lstrip() # обрезка пробелов слева\n",
    "    txt = txt.encode('ascii', 'ignore').decode() # удаляем все что не ascii символы   \n",
    "    txt = txt.lower() # текст в нижний регистр\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572547db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# общая информация о фильмах\n",
    "subtitles = [] \n",
    "\n",
    "# дополнительная информация о фильмах с новыми features\n",
    "subtitles_info = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0841a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = pyprind.ProgBar(df_movies.shape[0])\n",
    "\n",
    "def read_subs(row):\n",
    "    \"\"\"\n",
    "    Пофайловое чтение субтитров\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    row - объект DataFrame'а\n",
    "    \n",
    "    Результат:\n",
    "    ----------\n",
    "    row - объект DataFrame'а\n",
    "    \"\"\"\n",
    "    \n",
    "    movie = row.name\n",
    "    level = row['Max_Level']\n",
    "    path = row['Path']\n",
    "    \n",
    "    subs = None\n",
    "    \n",
    "    encodings = ['utf-8', 'ansi', 'utf-16-le']\n",
    "    \n",
    "    # специально пробегаем по всем кодировкам, можно добавить своё\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            subs = pysrt.open(path, encoding=encoding)\n",
    "            if len(subs) > 0:\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    try:\n",
    "        subtitles.append([movie, level, clean_subs(subs)])\n",
    "        \n",
    "        for i in range(len(subs)):\n",
    "            if i == 0:\n",
    "                # пропускаем первый рекламный субтитр\n",
    "                continue\n",
    "            len_ms = subs[i].end - subs[i].start\n",
    "            if len_ms.seconds > 0:\n",
    "                len_ms = (len_ms.seconds * 1000) + len_ms.milliseconds\n",
    "            else:\n",
    "                len_ms = len_ms.milliseconds\n",
    "                \n",
    "            subtitles_info.append([movie, level, subs[i].start, subs[i].end, subs[i].end - subs[i].start, len_ms])\n",
    "        \n",
    "    except Exception as e:\n",
    "        # обычно это связано с кодировкой\n",
    "        print(f'Ошибка чтения файла {path}: {e}')\n",
    "    \n",
    "    pbar.update()\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540252c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = df_movies.apply(read_subs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175dc685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаём DataFrame\n",
    "df_subtitles_info = pd.DataFrame(subtitles_info, columns=['movie', 'level', 'start', 'end', 'length', 'milliseconds'])\n",
    "\n",
    "df_subtitles_info.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10a7116c",
   "metadata": {},
   "source": [
    "Описание полей таблицы:\n",
    "* start - время старта субтитра\n",
    "* end - время завершения субтитра\n",
    "* length - время показа субтитра\n",
    "* milliseconds - время показа в миллисекундах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb7edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtitles = pd.DataFrame(subtitles, columns=['movie', 'level', 'text'])\n",
    "df_subtitles.set_index('movie', inplace=True)\n",
    "\n",
    "df_subtitles.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "592b4294",
   "metadata": {},
   "source": [
    "Описание полей таблицы:\n",
    "* level - уровень знаний английского языка\n",
    "* text - прочитанные субтитры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c03dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сгуппируем данные для генерации новых features для фильма\n",
    "df_subtitles_info = df_subtitles_info.groupby(['movie']).aggregate({'start': 'min', 'end': 'max', 'milliseconds': 'mean'})\n",
    "\n",
    "df_subtitles_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e0ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time2ms(time):\n",
    "    return (time.to_time().hour * 60 * 60) + (time.to_time().minute * 60) + (time.to_time().second)\n",
    "\n",
    "# специально сделал преобразование до секунд\n",
    "df_subtitles_info['len_sec'] = df_subtitles_info['end'].apply(time2ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c946a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# дополним фильмы новыми features\n",
    "df_subtitles = df_subtitles.merge(df_subtitles_info, how='left', left_index=True, right_index=True)\n",
    "df_subtitles.drop(columns=['start', 'end'], inplace=True)\n",
    "\n",
    "df_subtitles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc4c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# считываем словарь и создаём дополнительный объект\n",
    "df_words = pd.read_excel(WORDS_PATH)\n",
    "df_words = df_words.groupby(['level', 'word'])['file'].count().reset_index()\n",
    "\n",
    "dict_words = {}\n",
    "\n",
    "for level in df_words['level'].unique():\n",
    "    # создаём новые колонки\n",
    "    df_subtitles[level] = 0\n",
    "    \n",
    "    dict_words[level] = df_words.loc[df_words['level'] == level, 'word'].values\n",
    "    \n",
    "print(f'Были созданы колонки {\", \".join(df_words[\"level\"].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76abed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = pyprind.ProgBar(df_subtitles.shape[0])\n",
    "\n",
    "def set_level_count(row):\n",
    "    \"\"\"\n",
    "    Устанавливаем доли слов определённых категорий в фильме\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    row - объект DataFrame'а\n",
    "    \n",
    "    Результат:\n",
    "    ----------\n",
    "    row - объект DataFrame'а\n",
    "    \"\"\"\n",
    "    words = word_tokenize(row['text'])\n",
    "    \n",
    "    pbar.update()\n",
    "    \n",
    "    for level in df_words['level'].unique():\n",
    "        row[level] = len([word for word in words if word.lower() in dict_words[level]]) / len(words)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0acd271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# процесс обрабтки занимает некоторое время, всё зависит от количество фильмов\n",
    "df_subtitles = df_subtitles.apply(set_level_count, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f10b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = pyprind.ProgBar(df_subtitles.shape[0])\n",
    "\n",
    "all_stopwords = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    \"\"\"\n",
    "    Нормализация слов\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    text: string - входная строка\n",
    "    \n",
    "    Результат:\n",
    "    ----------\n",
    "    string - преобразованная строка\n",
    "    \"\"\"\n",
    "    pbar.update()\n",
    "    \n",
    "    if text == text:\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        # чистим от стоп-слов\n",
    "        words = word_tokenize(text)\n",
    "        words = [word for word in words if word.casefold() not in all_stopwords]\n",
    "\n",
    "        return \" \".join([porter.stem(word) for word in words])\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe41e5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# выполняем нормализацию данных\n",
    "df_subtitles['porter_text'] = df_subtitles['text'].apply(tokenizer_porter)\n",
    "\n",
    "df_subtitles.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8059ed1",
   "metadata": {},
   "source": [
    "Итоговая талица содержит следующую информацию:\n",
    "* level - уровень знания языка\n",
    "* text - оригинальные субтитры\n",
    "* milliseconds - среднее время показа субтитра\n",
    "* len_sec - продолжительность фильма в секундах\n",
    "* A1 - C1 - доля слов в каждой категории\n",
    "* porter_text - преобразованные субтитры"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae346e83",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b086837",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_subtitles[['porter_text', 'A1', 'A2', 'B1', 'B2', 'C1']]\n",
    "target = df_subtitles['level']\n",
    "\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(features, target, test_size=0.25, random_state=12345, shuffle=True)\n",
    "\n",
    "# разделил исходные данные\n",
    "print(f'Обучающая выборка:', features_train.shape[0])\n",
    "print(f'Валидационная выборка:', features_valid.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0615ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = ['A1', 'A2', 'B1', 'B2', 'C1']\n",
    "\n",
    "features_train_ohe = features_train.copy()\n",
    "features_valid_ohe = features_valid.copy()\n",
    "\n",
    "# приводим числовые значения к диапазону от 0 и до 1 (одна из моделей работает только со значениями этого диапазона)\n",
    "scaler = MinMaxScaler()\n",
    "features_train_ohe[numeric] = scaler.fit_transform(features_train_ohe[numeric])\n",
    "features_valid_ohe[numeric] = scaler.transform(features_valid_ohe[numeric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b216be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# используем викторизацию\n",
    "tfidf = TfidfVectorizer(lowercase=False)\n",
    "column_transformer = ColumnTransformer([('vect1', tfidf, 'porter_text')], remainder='passthrough')\n",
    "\n",
    "features_train_ohe = column_transformer.fit_transform(features_train_ohe)\n",
    "features_valid_ohe = column_transformer.transform(features_valid_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8cac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "best_cls = None\n",
    "best_params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8b4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(clf, params, custom_name=False):\n",
    "    \"\"\"\n",
    "    Специальный метод для определения наилучшей модели\n",
    "    \"\"\"\n",
    "    print(\"_\" * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    \n",
    "    gs_clf = GridSearchCV(clf, params,\n",
    "                           scoring='f1_weighted',\n",
    "                           cv=5,\n",
    "                           verbose=False,\n",
    "                           n_jobs=-1)\n",
    "    \n",
    "    gs_clf.fit(features_train_ohe, target_train)\n",
    "    \n",
    "    print('Best parameter set: %s ' % gs_clf.best_params_)\n",
    "    print('CV F1: %.3f' % gs_clf.best_score_)\n",
    "    \n",
    "    train_time = time() - t0\n",
    "    print(f\"train time: {train_time:.3}s\")\n",
    "\n",
    "    t0 = time()\n",
    "    \n",
    "    best_clf = gs_clf.best_estimator_\n",
    "    pred = best_clf.predict(features_valid_ohe)\n",
    "    test_time = time() - t0\n",
    "    print(f\"test time:  {test_time:.3}s\")\n",
    "\n",
    "    score = metrics.f1_score(target_valid, pred, average='weighted')\n",
    "    \n",
    "    global best_cls\n",
    "    global best_score\n",
    "    global best_params\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_cls = best_clf\n",
    "        best_score = score\n",
    "        best_params = gs_clf.best_params_\n",
    "        \n",
    "    print(f\"Valid F1:   {score:.3}\")\n",
    "\n",
    "    if hasattr(clf, \"coef_\"):\n",
    "        print(f\"dimensionality: {best_clf.coef_.shape[1]}\")\n",
    "        print(f\"density: {density(best_clf.coef_)}\")\n",
    "        print()\n",
    "\n",
    "    print()\n",
    "    if custom_name:\n",
    "        clf_descr = str(custom_name)\n",
    "    else:\n",
    "        clf_descr = best_clf.__class__.__name__\n",
    "    return clf_descr, score, train_time, test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bbabf4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "STATE=12345\n",
    "JOB=-1\n",
    "for clf, params, name in (\n",
    "    (LogisticRegression(penalty='l2', max_iter=1000, class_weight='balanced', random_state=STATE, n_jobs=JOB), {'C': [1, 3, 5, 7], 'solver': ['lbfgs', 'liblinear', 'sag', 'saga']}, \"Logistic Regression\"),\n",
    "    (RidgeClassifier(class_weight='balanced', random_state=STATE), {'alpha': [1, 3, 5, 7], 'solver': ['lsqr', 'sparse_cg', 'sag']}, \"Ridge Classifier\"),\n",
    "    (KNeighborsClassifier(n_jobs=JOB), {'n_neighbors': [50, 100, 150], 'leaf_size': [20, 30, 40]}, \"kNN\"),\n",
    "    (RandomForestClassifier(class_weight='balanced', random_state=STATE, n_jobs=JOB), {'n_estimators': [100, 200, 300], 'criterion': ['gini', 'entropy', 'log_loss'], 'max_depth': [5, 10, 15, 20]}, \"Random Forest\"),\n",
    "    # L2 penalty Linear SVC\n",
    "    (LinearSVC(class_weight='balanced', loss='squared_hinge', dual=False, random_state=STATE), {'C': [0.1, 0.5, 1.0]}, \"Linear SVC\"),\n",
    "    # L2 penalty Linear SGD\n",
    "    (SGDClassifier(alpha=1e-4, early_stopping=True, class_weight='balanced', random_state=STATE, n_jobs=JOB), {'n_iter_no_change': [3, 5, 7], 'loss': ['hinge', 'log_loss', 'log', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']}, \"SGD\"),\n",
    "    # NearestCentroid (aka Rocchio classifier)\n",
    "    (NearestCentroid(), {'metric': ['euclidean']}, \"NearestCentroid\"),\n",
    "    # Sparse naive Bayes classifier\n",
    "    (ComplementNB(), {'alpha': [0.1, 0.3, 0.5, 0.7, 0.9], 'norm': [True, False]}, \"Complement naive Bayes\")\n",
    "    ):\n",
    "    print(\"=\" * 80)\n",
    "    print(name)\n",
    "    print(params)\n",
    "    results.append(benchmark(clf, params, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817a8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Модель {best_cls.__class__.__name__} показала наилучший результат метрики F1 = {best_score}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b188a09",
   "metadata": {},
   "source": [
    "<b>Вывод</b>: для работы с текстовыми данными лучшие показатели были при применении векторизации слов <b>TfidfVectorizer</b>. При этом на повышение метрики хорошо повлияли доли слов в каждой категории."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6590abdf",
   "metadata": {},
   "source": [
    "## Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d4481",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_final = features.copy()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features_final[numeric] = scaler.fit_transform(features_final[numeric])\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase=False)\n",
    "column_transformer = ColumnTransformer([('vect1', tfidf, 'porter_text')], remainder='passthrough')\n",
    "\n",
    "features_final = column_transformer.fit_transform(features_final)\n",
    "\n",
    "dump(scaler, OUTPUT_PATH + 'subtitle.scaler')\n",
    "dump(column_transformer, OUTPUT_PATH + 'subtitle.transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde3190",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cls.fit(features_final, target)\n",
    "dump(best_cls, OUTPUT_PATH + 'subtitle.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
